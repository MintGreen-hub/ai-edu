
## 12.3 核函数的概念

### 12.3.1 更高维的映射与内积计算

在 12.1 小节中，$x$ 从一维的 $[x_1]$ 变成了二维 $[x_1, x_1^2]$，映射函数为 $\phi(z) = [z, z^2]$。

在 12.2 小节中，$x$ 从二维的 $[x_{1}, x_{2}]$ 变成了三维的 $[x_{1}, x_{2}, x_{1}^2+x_{2}^2]$，映射函数为 $\phi(z) = [z_1, z_2, z_1^2+z_2^2]$。当然，也可以定义 $\phi(z)=[z_1,z_2,z_1^2,z_2^2,z_1 z_2]$，那就变成了五维特征。

现在看看三维的例子，然后可以推广到一般情况。

当样本有三维特征时，其一般形式为：$\boldsymbol{z}=[z_1,z_2,z_3]$。下面定义只包括二次项的映射函数：

$$
\phi(z)=[z_1 z_1,\ z_1 z_2,\ z_1 z_3,\ z_2 z_1,\ z_2 z_2,\ z_2 z_3,\ z_3 z_1,\ z_3 z_2,\ z_3 z_3]
\tag{12.3.1}
$$

其中 $z_i z_j$ 表示 $z_i$ 乘以 $z_j$。

现在有两个样本：$\boldsymbol{x}_i=[1,2,3]$，$\boldsymbol{x}_j=[4,5,6]$。经过式 12.3.1 的映射后变成：

$\phi(\boldsymbol{x}_i)=[1,2,3,2,4,6,3,6,9]$

$\phi(\boldsymbol{x}_j)=[16,20,24,20,25,30,24,30,36]$


现在计算 $\phi(\boldsymbol{x}_i) \cdot \phi(\boldsymbol{x}_j)$ 的内积......稍等，有的读者会问：
1. 为什么要计算内积？
2. 为什么要计算映射函数 $\phi(\boldsymbol{x}_i)$ 和 $\phi(\boldsymbol{x}_j)$ 的内积？

因为式 SVM 的优化目标函数如下：
$$
D(\alpha)=\sum_{i=1}^n\alpha_i-\frac{1}{2}\sum_{i=1}^n\sum_{j=1}^n\alpha_i \alpha_j y_i y_j (\boldsymbol{x}_i \cdot \boldsymbol{x}_j) \tag{12.3.2}
$$

它需要计算样本数据 $(\boldsymbol{x}_i \cdot \boldsymbol{x}_j)$ 的内积，这就回答了第 1 个问题，为什么要计算内积。

那为什么要计算映射函数 $\phi(\boldsymbol{x}_i)$ 和 $\phi(\boldsymbol{x}_j)$ 的内积呢？因为从 11.4 和 11.5 的两个例子中，我们已经得到了一个初步的认识：当样本在原始空间线性不可分时，如果映射到高维空间，是有可能变成线性可分的；而且维数越高，线性可分的可能性越大。**在映射之后，就可以使用 SVM 来做线性分类了，只不过要把原来的 $(\boldsymbol{x}_i \cdot \boldsymbol{x}_j)$ 替换成 $\phi(\boldsymbol{x}_i) \cdot \phi(\boldsymbol{x}_j)$。**

好，可以继续了，接下来计算内积：
$$
\begin{aligned}
\phi(\boldsymbol{x}_i ) \cdot \phi(\boldsymbol{x}_j )&=[1,2,3,2,4,6,3,6,9] \cdot [16,20,24,20,25,30,24,30,36]
\\\\
&=1 \times 16 + 2  \times 20 + 3  \times 24 + 2  \times 20 + 4  \times 25 + 6  \times 30 + 3  \times 24 + 6  \times 30 + 9  \times 36
\\\\
&=16+40+72+40+100+180+72+180+324
\\\\
&=1024
\end{aligned}
\tag{12.3.3}
$$


可以看到式 12.3.3 的计算过程相当繁琐：
1. 首先要把原始是三维特征映射为九维特征。
2. 然后要在高维空间（九维）中计算内积。

实现上述过程的代码如下：

```python
def f(z):
    n = z.shape[1]
    Z = np.zeros(shape=(1,n*n))
    # 生成九维特征
    for i in range(3):
        for j in range(3):
            Z[0,i*3+j] = z[0,i]*z[0,j]
    return Z
```
计算映射后的内积计算代码如下：

```python
def fx_fy(x,y):
    fx = f(x)
    fy = f(y)
    result = np.inner(fx, fy)
    return result
```

幸好有np.inner() 函数可以帮助我们计算两个矢量的内积，否则要还要手工实现。

实际上，我们在代码 Code_11_4_1_1D.py 中已经实践过一次上述过程了：

```python
    X = transform(X_raw)
    model = linear_svc(X, Y)
```

第一行的 transform() 函数，就是做了高维特征映射；第二行代码把新的 X 特征样本送进了线性 SVM 分类器中做内积运算，从而得到模型 model。

### 12.3.2 核函数的出现

我们对式 12.3.3 进行一下简单的反向推导：

$$
\begin{aligned}
\phi(\boldsymbol{x}_i) \cdot \phi(\boldsymbol{x}_j) &= [x_{i,1} x_{i,1},x_{i,1} x_{i,2},x_{i,1} x_{i,3},x_{i,2} x_{i,1},x_{i,2} x_{i,2},x_{i,2} x_{i,3},x_{i,3} x_{i,1},x_{i,3} x_{i,2},x_{i,3} x_{i,3}]
\\\\
& \cdot [x_{j,1} x_{j,1},x_{j,1} x_{j,2},x_{j,1} x_{j,3},x_{j,2} x_{j,1},x_{j,2} x_{j,2},x_{j,2} x_{j,3},x_{j,3} x_{j,1},x_{j,3} x_{j,2},x_{j,3} x_{j,3}]
\\\\
&=x_{i,1}^2 x_{j,1}^2 + x_{i,2}^2 x_{j,2}^2 + x_{i,3}^2 x_{j,3}^2 + 2 x_{i,1} x_{j,1} x_{i,2} x_{j,2}+ 2 x_{i,1} x_{j,1} x_{i,3} x_{j,3} + 2 x_{i,2} x_{j,2} x_{i,3} x_{j,3}
\\\\
&=(x_{i,1} x_{j,1} + x_{i,2} x_{j,2} + x_{i,3} x_{j,3})^2
\\\\
&=[(x_{i,1},x_{i,2},x_{i,3}) \cdot (x_{j,1}, x_{j,2}, x_{j,3})]^2
\\\\
&=(\boldsymbol{x}_i \cdot \boldsymbol{x}_j)^2
\end{aligned}
\tag{12.3.4}
$$

所以，如果令：

$$
K(\boldsymbol{x}_i,\boldsymbol{x}_j)=(\boldsymbol{x}_i \cdot \boldsymbol{x}_j)^2 \tag{12.3.5}
$$

则式 12.3.5 就是核函数的定义。

将 $\boldsymbol{x}_i、\boldsymbol{x}_j$ 的具体值带入式 12.3.5 进行验证：

$$
\begin{aligned}   
K(\boldsymbol{x}_i,\boldsymbol{x}_j)&=K([1,2,3],[4,5,6])=([1,2,3] \cdot [4,5,6])^2
\\\\
&=(1 \times 4 + 2 \times 5 + 3 \times 6)^2 = (4+10+18)^2=32^2
\\\\
&=1024
\end{aligned}
\tag{12.3.5}
$$

结果与式 12.3.3 的计算结果相同。

用代码直接实现核函数的话，会非常简单：

```python
def k(x,y):
    result = np.inner(x, y)**2
    return result
```

可以做一个对比，分别调用 12.3.1 小节中的 fx_fy(x,y) 函数和上面的 k(x,y) 函数各 100000 次，在笔者的电脑上，运行代码 Code_11_6_1_mapping.py，得到如下运行信息：

```
原始输入维数: 3
映射空间维数: 9
[[1024.]] [[1024]]
fx_fy(x,y)运行时间: 1.4461944103240967
K(x,y)运行时间: 0.190537691116333
```

- 首先获得原始特征维数为 3，而映射后的特征维数为 9；
- 其次确定二者的计算结果都是 1024，相等；
- 最后比较运行时间，用映射函数的内积运算，需要 1.4 秒；而直接计算核函数只需要 0.19 秒。后者比前者快一个数量级。这里有两个原因：

1. 函数 $\phi(z)$ 生成映射特征时需要花费时间；
2. 映射后从 3 维特征变成了 9 维特征，计算内积需要花费更多的时间。

以上结果请运行 Code_12_3_Mapping.py 得到。

### 12.3.3 核函数与映射函数的关系

核函数与映射函数并非是一一对应的，以 $K(x_i,x_j)=(x_i,x_i)^2$ 举例来说，映射函数可能是：

$$
\phi(z) = [z_1^2,\ \sqrt{2}z_1 z_2, \ z_2^2]
\tag{12.3.6}
$$

也可能是：

$$
\phi(z) = \frac{1}{\sqrt{2}} [z_1^2-z_2^2,\ 2z_1 z_2, \ z_1^2+z_2^2]
\tag{12.3.7}
$$

如果映射到四维空间，将会是：

$$
\phi(z)=[z_1^2, \ z_1 z_2, \ z_1 z_2,\ z_2^2]
\tag{12.3.8}
$$

从另一方面讲，$[x_{1},x_{2}]$ 可以：

- 映射成二维：$[x_{1}^2,\ x_{2}^2]$ 或 $[x_{1}^2+x_{2}^2,\ x_1 x_2]$
- 映射成三维：$[x_{1},\ x_{2},\ x_{1}^2+ x_{2}^2]$ 或 $[x_{1},\ x_{2},\ x_{1} x_{2}]$
- 映射成四维：$[x_{1}^2,\ x_{2}^2,\ x_{1},\ x_{2}]$
- 映射成五维：$[x_{1}^2,\ x_{2}^2,\ x_{1} x_{2},\ x_{1},\ x_{2}]$

所对应的核函数都不一样，有些有可能没有对应的核函数。

最麻烦的事情是，从一个 3 维的原始特征 $[x_{1},x_{2},x_{3}]$ 向量出发，如果只构造二次项的话，新的特征空间会有 9 维（式 12.3.1）；如果构造三次项的话，可以映射成 19 维的新特征空间：

$$
\phi(x)=[1,x_{1},x_{2},x_{3}, x_{1}^2,x_{2}^2,x_{3}^2,x_{1} x_{2}, x_{1} x_{3}, x_{2} x_{3},x_{1}^3,x_{2}^3,x_{3}^3,x_{1}^2 x_{2},x_{1}^2 x_{3},x_{2}^2 x_{1},x_{2}^2 x_{3},x_{3}^2 x_{1},x_{3}^2 x_{2}]
$$

第一个 1 相当于 $x_{1}^0 x_{2}^0 x_{3}^0$，即零次项；后面接一次项、二次项、三次项。

- 在生成二次项的时候（d=2），如果是两个特征（k=2），需要遍历 $2^2$ 次；
- 在生成二次项的时候（d=2），如果是三个特征值（k=3），需要遍历 $3^2$ 次；
- 在生成三次项的时候（d=3），如果是两个特征值（k=2），需要遍历 $2^3$ 次；

一般地，对于多项式核函数，如果 $x$ 是 $k$ 维空间特征，生成 $d$ 次项，需要遍历 $k^d$ 次，即算法复杂度为 $O(k^d)$，但是使用核函数计算的复杂度为 $O(k)$，即只与原始特征数量有关。

### 12.3.4 常用核函数

什么样的函数可以当作是核函数呢？

**Mercer 定理**：

任何半正定的函数都可以作为核函数。

所谓半正定的函数 $f(x_i,x_j)$，是指拥有训练数据集合 $(x_1,x_2,…x_n)$，定义一个矩阵的元素 $a_{ij}=f(x_i,x_j)$，这个矩阵是 $n×n$ 的。如果这个矩阵是半正定的，那么 $f(x_i,x_j)$ 就称为半正定的函数。

所谓半正定指的就是核矩阵 K 的特征值均为非负，核矩阵的定义如下：

$$
K_{n \times n}=
\begin{bmatrix}
K(x_1,x_1) & K(x_1,x_2) & \cdots & K(x_1,x_n)
\\\\
\vdots & \vdots & \ddots & \vdots
\\\\
K(x_n,x_1) & K(x_n,x_2) & \cdots & K(x_n,x_n)
\end{bmatrix}
$$

其中 $K(x_i,x_j) == K(x_j, x_i)$，所以核矩阵是一个对称矩阵。

Mercer 定理不是核函数必要条件，只是一个充分条件，即还有不满足Mercer定理的函数也可以是核函数。

表 12.3.1 常用核函数

|名称|表达式|说明|
|--|--|--|
|线性核|$\boldsymbol{x}_i \cdot \boldsymbol{x}_j  + c$|只能解决线性可分问题，不能把非线性问题转换成线性问题|
|多项式核|$[\gamma (\boldsymbol{x}_i \cdot \boldsymbol{x}_j) + c]^d$|$\gamma$ 缺省取值为特征数的倒数，$d$ 一般取值为 2，常用形式为 $[\gamma(\boldsymbol{x}_i \cdot \boldsymbol{x}_j)+1]^2$|
|高斯核|$e^{-\parallel \boldsymbol{x}_i-\boldsymbol{x}_j \parallel^2/2 \sigma^2}$ |径向基（Radial Basis Function）核，对于样本噪音有较好的抗干扰能力|
|高斯核变形|$e^{-\gamma \parallel \boldsymbol{x}_i-\boldsymbol{x}_j \parallel^2}$ | $\gamma$ 一般取值为特征数的倒数，值越大拟合能力越强，直至过拟合|
|双曲正切核|$\tanh(\gamma (\boldsymbol{x}_i \cdot \boldsymbol{x}_j +c))$|主要用于深度学习，$\gamma$ 缺省取值为特征数的倒数|
|指数核|$e^{-\parallel \boldsymbol{x}_i-\boldsymbol{x}_j \parallel /2 \sigma^2}$|高斯核函数的变种，它将向量范数从 2 调整为 1，这样改动会对参数 $\sigma$ 的依赖性降低，但是适用范围较窄|
|拉普拉斯核|$e^{- \parallel \boldsymbol{x}_i-\boldsymbol{x}_j \parallel)/\sigma}$|完全等价于指数核，区别在于对参数 $\sigma$ 的敏感性降低，也是一种径向基核函数|




- 如果特征的数量大到和样本数量差不多，则选用LR或者线性核的SVM；
- 如果特征的数量小，样本的数量正常，则选用SVM+高斯核函数；
- 如果特征的数量小，而样本的数量很大，则需要手工添加一些特征从而变成第一种情况。
- 实际上使用可以对多个核函数进行测试，选择表现效果最好的核函数。

说明：n为特征数，m为样本个数

（1）若n相对m比较大，如n=10000, m=10~1000, 使用logistic/ SVM（线性核）均可；

（2）若n较小， m中等大小，如n=1~1000, m=10~10000, 使用SVM高斯核；

（3）若n较小，m较大，如n=1~1000, m=50000+, 那么需增加特征，此时用多项式核或高斯核;

(4)更一般：m小，用简单模型；m大用复杂模型。


（1）linear核：无需设置参数；

（2）poly核 ：degree, gamma, coef0；

（3）brf核：gamma；

（4）sigmoid核：gamma, coef0；

